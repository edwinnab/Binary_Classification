{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "#download the dataset\n",
    "\n",
    "(train_data, test_data), info = tfds.load(\n",
    "    # Use the version pre-encoded with an ~8k vocabulary.\n",
    "    'imdb_reviews/subwords8k', \n",
    "    # Return the train/test datasets as a tuple.\n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
    "    as_supervised=True,\n",
    "    # Also return the `info` structure. \n",
    "    with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8185\n"
     ]
    }
   ],
   "source": [
    "#check the encoder\n",
    "encoder = info.features[\"text\"].encoder\n",
    "print(\"Vocabulary size: {}\".format(encoder.vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string is [4025, 222, 6307, 2327, 4043, 2120]\n",
      "The original string. \"Hello TensorFlow\"\n"
     ]
    }
   ],
   "source": [
    "#encorder reversibly encodes any string\n",
    "\n",
    "sample_string = \"Hello TensorFlow\"\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string. \"{}\"'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025 ----- Hell\n",
      "222 ----- o \n",
      "6307 ----- Ten\n",
      "2327 ----- sor\n",
      "4043 ----- Fl\n",
      "2120 ----- ow\n"
     ]
    }
   ],
   "source": [
    "for ts in encoded_string:\n",
    "    print('{} ----- {}'.format(ts, encoder.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: [  62   18   41  604  927   65    3  644 7968   21]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "#dataset comes preprocessed\n",
    "#each example in an array of integers rep the words of the main movie review\n",
    "#each label is either integer value 1 0r 0\n",
    "\n",
    "for train_example, train_label in train_data.take(1):\n",
    "    print(\"Encoded text:\", train_example[:10].numpy())\n",
    "    print(\"Label:\", train_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode(train_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape:  (32, 770)\n",
      "Label shape:  (32,)\n",
      "Batch shape:  (32, 1043)\n",
      "Label shape:  (32,)\n"
     ]
    }
   ],
   "source": [
    "#prepare the dat for training\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "train_batches = (\n",
    "    train_data\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32))\n",
    "test_batches = (\n",
    "    test_data\n",
    "    .padded_batch(32))\n",
    "\n",
    "#each batch has the shape (batch_size, sequence_length)\n",
    "\n",
    "for example_batch, label_batch in train_batches.take(2):\n",
    "    print(\"Batch shape: \", example_batch.shape)\n",
    "    print(\"Label shape: \", label_batch.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 130,977\n",
      "Trainable params: 130,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(encoder.vocab_size, 16),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(1)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6840 - accuracy: 0.5002 - val_loss: 0.6685 - val_accuracy: 0.4990\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.6268 - accuracy: 0.5442 - val_loss: 0.5999 - val_accuracy: 0.5760\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.5474 - accuracy: 0.6588 - val_loss: 0.5376 - val_accuracy: 0.6719\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.4803 - accuracy: 0.7445 - val_loss: 0.4881 - val_accuracy: 0.7875\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.4262 - accuracy: 0.7977 - val_loss: 0.4488 - val_accuracy: 0.7937\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3843 - accuracy: 0.8296 - val_loss: 0.4203 - val_accuracy: 0.8281\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.3526 - accuracy: 0.8509 - val_loss: 0.4004 - val_accuracy: 0.8260\n",
      "Epoch 8/10\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.3273 - accuracy: 0.8657"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model.compile(optimizer= \"adam\",\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "             metrics = [\"accuracy\"])\n",
    "#train the model\n",
    "\n",
    "history = model.fit(train_batches,\n",
    "                   epochs =10,\n",
    "                   validation_data = test_batches,\n",
    "                   validation_steps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "loss, accuracy = model.evaluate(test_batches)\n",
    "\n",
    "print(\"Loss\", loss)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the history object has a dict with everything that happend before\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#four entries each is monitored during training and validation\n",
    "#plot a training and validation loss for comparison\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
